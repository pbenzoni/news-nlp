{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 207 Final Project - Peter Benzoni\n",
    "\n",
    "## Data Setup\n",
    " \n",
    "### Load the data, creating the appropriate files if they dont already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "BALANCED_DATASET = False\n",
    "# Check if combined-news-articles.csv already exists:\n",
    "# If it does, load the dataframe from the csv file\n",
    "# If it doesn't, create the dataframe from the original datasets\n",
    "try:\n",
    "    df = pd.read_csv('combined-news-articles.csv')\n",
    "    BALANCED_DATASET = True\n",
    "\n",
    "except FileNotFoundError:\n",
    "    site_map = {\n",
    "        \"http://en.people.cn/\": \"People's Daily\",\n",
    "        \"http://global.chinadaily.com.cn/\": \"China Daily\",\n",
    "        \"http://www.globaltimes.cn/\": \"Global Times\",\n",
    "        \"http://www.xinhuanet.com/english/\": \"Xinhua\" ,\n",
    "        \"https://www.cgtn.com/\": \"CGTN\",\n",
    "        \"xinhuanet.com (english)\": \"Xinhua\",\n",
    "    }\n",
    "\n",
    "    # Load the datasets\n",
    "    df = pd.read_csv('all-the-news-2-1.csv')\n",
    "    df_china = pd.read_csv('China_Articles.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the Data to desired dates and publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BALANCED_DATASET:\n",
    "    # Filter out articles from specified publications, excluding tabloid, significantly slanted, analysis/long-form, and opinion publications (and including news-first publications)\n",
    "    included_publications = [\"Axios\", \"Business Insider\", \"CNBC\", \"CNN\", \"Fox News\", \"Reuters\", \"The Hill\", \"The New York Times\", \"Washington Post\",]\n",
    "    df = df[df['publication'].isin(included_publications)]\n",
    "\n",
    "    # filter to only articles from 7-2019 to 7-2020 to match df_china\n",
    "    df = df[((df['year'] == 2019) & (df['month'] >= 7)) | ((df['year'] == 2020) & (df['month'] <= 7))]\n",
    "\n",
    "    # filter out articles from specified publications for df_china\n",
    "    excluded_publications_china = ['china-un.org (english)']\n",
    "    df_china = df_china[~df_china['site'].isin(excluded_publications_china)]\n",
    "\n",
    "    # replace sites with publication names for df_china\n",
    "    df_china['publication'] = df_china['site'].map(site_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize formatting and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BALANCED_DATASET:\n",
    "    # match date columns for df_china to df columns (date, year, month)\n",
    "    df_china['date'] = pd.to_datetime(df_china['published'], format='mixed')\n",
    "    df_china['year'] = df_china['date'].dt.year\n",
    "    df_china['month'] = df_china['date'].dt.month\n",
    "\n",
    "    # Handling missing values - remove empty articles\n",
    "    df = df[df['title'].notna()]\n",
    "    df_china = df_china[df_china['title'].notna()]\n",
    "    df = df[df['article'].notna()]\n",
    "    df_china = df_china[df_china['excerpt'].notna()]    \n",
    "\n",
    "    # Convert the data into a format suitable for NLP analysis; Matching the first 1000 characters available in the other dataset\n",
    "    df['combined_content'] = df['title'].astype(str) + ': ' + df['article'].str[:1000].astype(str)\n",
    "    df_china['combined_content'] = df_china['title'].astype(str) + ': ' + df_china['excerpt'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance datasets\n",
    "For the \"all the news\" dataset: The smallest publication here is Buzzfeed News with 3,657 articles. To ensure that Axios is represented each month, set N to be around 600 (since 7674/12 = 614). \n",
    "\n",
    "For the Chinese state media dataset: The smallest publication is Global Times with 5,242 articles. To ensure representation each month, set N to around 430 (since 5242/12 = 437).\n",
    "\n",
    "That way, the data should be balanced by publication and date. We could balance by actor as well (china vs non-china, but its useful to have a boarder context the chinese media fits into) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication\n",
      "Reuters               202042\n",
      "CNBC                   71754\n",
      "Business Insider       48583\n",
      "The New York Times     42655\n",
      "Washington Post        40483\n",
      "The Hill               37940\n",
      "CNN                    29921\n",
      "Axios                   7374\n",
      "Name: count, dtype: int64\n",
      "publication\n",
      "Xinhua            56695\n",
      "CGTN              17820\n",
      "People's Daily    10701\n",
      "China Daily        9417\n",
      "Global Times       5242\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if not BALANCED_DATASET:\n",
    "    # get and print length of each publication , to justify balancing the datasets\n",
    "    print(df['publication'].value_counts())\n",
    "    print(df_china['publication'].value_counts())\n",
    "    \n",
    "    balanced_df = pd.DataFrame()\n",
    "    balanced_df_china = pd.DataFrame()\n",
    "\n",
    "    for year in df['year'].unique():\n",
    "        for month in df['month'].unique():\n",
    "            # Sample N articles from each publication in df for the specific month and year\n",
    "            N = 610\n",
    "            temp_df = df[(df['year'] == year) & (df['month'] == month)]\n",
    "            sampled_df = temp_df.groupby('publication').apply(lambda x: x.sample(min(len(x), N))).reset_index(drop=True)\n",
    "            balanced_df = pd.concat([balanced_df, sampled_df], ignore_index=True)\n",
    "            \n",
    "            # Do the same for df_china\n",
    "            N = 430\n",
    "            temp_df_china = df_china[(df_china['year'] == year) & (df_china['month'] == month)]\n",
    "            sampled_df_china = temp_df_china.groupby('publication').apply(lambda x: x.sample(min(len(x), N))).reset_index(drop=True)\n",
    "            balanced_df_china = pd.concat([balanced_df_china, sampled_df_china], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Balanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publication\n",
      "The New York Times    5546\n",
      "Business Insider      5497\n",
      "CNBC                  5490\n",
      "CNN                   5490\n",
      "Reuters               5490\n",
      "The Hill              5490\n",
      "Axios                 3661\n",
      "Washington Post       3290\n",
      "Xinhua                3061\n",
      "CGTN                  2929\n",
      "People's Daily        2770\n",
      "China Daily           2753\n",
      "Global Times          2729\n",
      "Name: count, dtype: int64\n",
      "                  date  year  month            author  \\\n",
      "0  2019-07-23 00:00:00  2019    7.0  Kia Kokalitcheva   \n",
      "1  2019-07-11 00:00:00  2019    7.0      Erica Pandey   \n",
      "2  2019-07-15 00:00:00  2019    7.0       Dan Primack   \n",
      "3  2019-07-23 00:00:00  2019    7.0     Alayna Treene   \n",
      "4  2019-07-11 00:00:00  2019    7.0         Ben Geman   \n",
      "\n",
      "                                               title publication  \\\n",
      "0              DoorDash's \"tips\" model is under fire       Axios   \n",
      "1  Amazon's competitors will see their revenue ju...       Axios   \n",
      "2     AB InBev cancels $9 billion-plus Hong Kong IPO       Axios   \n",
      "3  What Nancy Pelosi is telling House Democrats a...       Axios   \n",
      "4  Elizabeth Warren wants public companies to iss...       Axios   \n",
      "\n",
      "                                    combined_content  \n",
      "0  DoorDash's \"tips\" model is under fire: DoorDas...  \n",
      "1  Amazon's competitors will see their revenue ju...  \n",
      "2  AB InBev cancels $9 billion-plus Hong Kong IPO...  \n",
      "3  What Nancy Pelosi is telling House Democrats a...  \n",
      "4  Elizabeth Warren wants public companies to iss...  \n"
     ]
    }
   ],
   "source": [
    "if not BALANCED_DATASET:\n",
    "\n",
    "    # Combine the dataframes, keeping only the intersection of columns\n",
    "    df = pd.concat([balanced_df, balanced_df_china], axis=0, join='inner')\n",
    "    print(df['publication'].value_counts())\n",
    "    \n",
    "\n",
    "    # Now, df['combined_content'] can be used for NLP analysis\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the dataframe to a csv file for future use\n",
    "    df.to_csv('combined-news-articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Start by installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\benzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\benzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\benzo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Tokenization and Stop-word removal\n",
    "def preprocess(documents):\n",
    "    # Lowercasing\n",
    "    documents = [doc.lower() for doc in documents]\n",
    "\n",
    "    # Tokenization\n",
    "    documents_tokens = [nltk.word_tokenize(doc) for doc in documents]\n",
    "\n",
    "    # Stop Words Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    documents_tokens = [[word for word in doc if word not in stop_words] for doc in documents_tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    documents_lemmatized = [[lemmatizer.lemmatize(word) for word in doc] for doc in documents_tokens]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    documents_stemmed = [[stemmer.stem(word) for word in doc] for doc in documents_tokens]\n",
    "\n",
    "    # Removing Numbers and Punctuation\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation + string.digits))\n",
    "    documents_tokens = [[regex.sub('', word) for word in doc] for doc in documents_tokens]\n",
    "    documents_tokens = [[word for word in doc if word] for doc in documents_tokens]  # Remove empty strings\n",
    "\n",
    "    # TF-IDF Transformation\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform([' '.join(doc) for doc in documents_tokens])\n",
    "\n",
    "    # Removing Frequent and Rare Words\n",
    "    freq_thresh = 0.95\n",
    "    rare_thresh = 0.05\n",
    "    word_freq = X.sum(axis=0).tolist()[0]\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    features_list = list(features)\n",
    "    docs_filtered = [[word for word in doc if word in features_list and word_freq[features_list.index(word)] < freq_thresh and word_freq[features_list.index(word)] > rare_thresh] for doc in documents_tokens]\n",
    "\n",
    "    # Removing Short Words\n",
    "    documents_tokens = [[word for word in doc if len(word) > 2] for doc in documents_tokens]\n",
    "\n",
    "    # Bi-grams and N-grams\n",
    "    bigram = Phrases(documents_tokens, min_count=5, threshold=50)\n",
    "    documents_bigrams = [bigram[doc] for doc in documents_tokens]\n",
    "\n",
    "    # Word Embeddings - save this for later?\n",
    "    # model = Word2Vec(sentences=documents_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    # model.save(\"word2vec.model\")\n",
    "\n",
    "\n",
    "    return documents_tokens, documents_bigrams\n",
    "\n",
    "documents_tokens, documents_bigrams = preprocess(df['combined_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "# Before we pick how many topics we want to model, we can use the coherence score to evaluate the model\n",
    "# Comment this out after the first run, as it takes a while to run\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=df['tokens'], start=5, limit=50, step=5)\n",
    "# print(coherence_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Single word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(documents_tokens)\n",
    "\n",
    "# Filter out words that occur in less than 20 documents, or more than 70% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.7)\n",
    "\n",
    "# Convert the dictionary into a bag-of-words corpus.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents_tokens]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 25  \n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "\n",
    "# Print the topics\n",
    "pprint.pprint(lda.print_topics(num_words=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents with bigrams.\n",
    "dictionary_bigrams = Dictionary(documents_bigrams)\n",
    "\n",
    "# Filter out words that occur in less than 20 documents, or more than 70% of the documents.\n",
    "dictionary_bigrams.filter_extremes(no_below=20, no_above=0.7)\n",
    "\n",
    "# Convert the dictionary into a bag-of-words corpus.\n",
    "corpus_bigrams = [dictionary_bigrams.doc2bow(doc) for doc in documents_bigrams]\n",
    "\n",
    "# Train the LDA model on the bigram corpus\n",
    "num_topics = 25 \n",
    "lda_bigrams = LdaModel(corpus=corpus_bigrams, id2word=dictionary_bigrams, num_topics=num_topics, random_state=42)\n",
    "\n",
    "# Print the topics for the model trained on bigrams\n",
    "pprint.pprint(lda_bigrams.print_topics(num_words=15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
